[
{"author": "anonymous", "text": "By decoding both the voice and the images as separate objects from a video (but from requests to the same model), you can get an even better understanding of a video than either approach alone, for whatever analysis, summary etc you want to perform."},
{"author": "anonymous2", "text": "We need to know how the vision works, its super fuzzy in the demos. Does it see video live? Does it take an image? Can it mix the audio and vision at the same time?"},
{"author": "userX", "text": "I used it via the API so not multimodal, but reasoning seems to be much improved. I asked it my go-to question: 'Assume today is Wednesday. What weekday was it eight days ago? Answer first, then work it out.' Many, many models mess this up in various ways. GPT4o nailed it. Some other trick questions too. Very, very promising. But we need to be able to change that voice."}
]